{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from pprint import pprint\n",
    "from peer_review import *\n",
    "from vancouver_simulations import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation Parameters\n",
    "\n",
    "The simulations below were run with the following default parameters:\n",
    "- There are twenty group submissions.\n",
    "- There are three students per group.\n",
    "- Each student grades three assignments.\n",
    "- The simulation is run fifty times and the data aggregated.\n",
    "- The legend indicates the number of ground-truth grades supplied to the algorithm.\n",
    "- The default method for choosing ground truth grades is to select them uniformly at random.\n",
    "- Methods for choosing ground truth grades are applied after the entire cover has been chosen. If the cover is smaller than the number of ground truth grades allowed, the grades used are chosen uniformly at random from those in the cover.\n",
    "- Peer quality is represented by the number of draws a peer gets from a uniform distribution on the range (0, 1).\n",
    "- Peer quality is uniformly random on the range (1, 5).\n",
    "- The true value of a submission's grade is always 0.5, the expectation of a uniform distribution on the range (0, 1).\n",
    "- The grading algorithm used is the Vancouver algorithm, and it is terminated after ten iterations.\n",
    "- The statistic plotted is the CDF of submission grade error, the quantity abs(submission grade from algorithm - 0.5).\n",
    "- The default number of ground truths is the tuple (0, 5, 10, 15) and should plot four CDFs per plot.\n",
    "- Each plot runs its own batch of simulations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_cdfs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-Evaluation of the Algorithms\n",
    "\n",
    "Below follows a re-evaluation of the two algorithms for choosing ground truths past the cover (greedy by highest grade error and greedy by highest submission variance) that were previously evaluated. As the submission variances are not modeled by our input to the algorithm, it makes sense that the second algorithm would show no noticable improvements over random selection past the cover. The first algorithm showed no clearly visible improvements in previous trials, but is re-evaluated here with higher precision and the new plotting tool for showing multiple CDFs on the same plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def highest_grade_error(t, init, actual):\n",
    "    scores = init[0]\n",
    "    qualities = init[1]\n",
    "    omni_scores = actual[0]\n",
    "    true_qualities = actual[1]\n",
    "    \n",
    "    sub_score_error = [abs(scores[submission][0] - 0.5) for submission in scores]\n",
    "    sub_var_error = [abs(scores[submission][1] - omni_scores[submission][1]) for submission in scores]\n",
    "    grader_var_error = [abs(qualities[grader] - true_qualities[grader]) for grader in qualities]\n",
    "    \n",
    "    return max(sub_grade_error.iteritems(), key=operator.itemgetter(1))[0]\n",
    "\n",
    "def highest_submission_variance(t, init, actual):\n",
    "    scores = init[0]\n",
    "    qualities = init[1]\n",
    "    omni_scores = actual[0]\n",
    "    true_qualities = actual[1]\n",
    "    \n",
    "    sub_score_error = [abs(scores[submission][0] - 0.5) for submission in scores]\n",
    "    sub_var_error = [abs(scores[submission][1] - omni_scores[submission][1]) for submission in scores]\n",
    "    grader_var_error = [abs(qualities[grader] - true_qualities[grader]) for grader in qualities]\n",
    "    \n",
    "    sub_var = [scores[submission][1] for submission in scores]\n",
    "    \n",
    "    return max(sub_var.iteritems(), key=operator.itemgetter(1))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_cdfs(grading_algorithm=highest_grade_error, num_trials=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_cdfs(grading_algorithm=highest_submission_variance, num_trials=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here is the random method for comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_cdfs(num_trials=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As is evident from the above, there appears to be no noticable difference between the algorithms under these simulation parameters. It is possible that the introduction of submission variance could cause the second algorithm to perform better. In more general analysis of the graphs, it is clear that the y-axis intercept represents the percentage directly graded with ground truth. The plots then asymptotically approach one. It does not appear that increasing the number of ground truth grades in any instance causes excessive change in this pattern; the plot is closer to the asymptote faster, but that appears to be because it starts closer to it, rather than due to any improvement in the non-ground-truth scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
