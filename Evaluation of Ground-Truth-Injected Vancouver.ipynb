{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from pprint import pprint\n",
    "from peer_review import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def evaluate_vancouver(num_assignments, num_reviews, num_truths, min_quality=1, max_quality=5, use_cover=True,\n",
    "                       vancouver_steps=10):\n",
    "    # generate random groups, assignments, qualities, and reviews\n",
    "    groups = { sub : [sub + x for x in ['1','2','3']] for sub in [ chr(ord('a') + z) for z in range(num_assignments)]}\n",
    "    assignments, cover = peer_assignment(groups, num_reviews)\n",
    "    true_qualities = {i: random.randint(min_quality, max_quality) for i in assignments}\n",
    "    reviews = random_reviews(assignments, true_qualities)\n",
    "    \n",
    "    # generate a random ground truth value for all submissions\n",
    "    truths = {i: 0.5 for i in groups}\n",
    "    \n",
    "    # make a truths_visible dictionary for the algorithm to have access to\n",
    "    if use_cover:\n",
    "        if len(cover) > num_truths:\n",
    "            truths_visible = {i[0]: 0.5 for i in random.sample(list(cover), num_truths)}\n",
    "        else:\n",
    "            while len(cover.keys()) < num_truths:\n",
    "                cover[random.choice(truths)] = 0.5\n",
    "            truths_visible = cover\n",
    "    else:\n",
    "        truths_visible = {i[0]: 0.5 for i in random.sample(list(truths), num_truths)}\n",
    "\n",
    "    # run vancouver and omniscient vancouver\n",
    "    scores, qualities = vancouver(reviews, truths_visible, vancouver_steps)\n",
    "    omni_scores, omni_qualities = vancouver(reviews, truths, vancouver_steps)\n",
    "    \n",
    "    # generate statistics on the data\n",
    "    sub_score_error = [abs(scores[submission][0] - 0.5) for submission in scores]\n",
    "    sub_var_error = [abs(scores[submission][1] - omni_scores[submission][1]) for submission in scores]\n",
    "    grader_var_error = [abs(qualities[grader] - true_qualities[grader]) for grader in qualities]\n",
    "\n",
    "    return sub_score_error, sub_var_error, grader_var_error\n",
    "\n",
    "    \n",
    "def vancouver_statistics(num_assignments, num_reviews, num_truths, num_runs, min_quality=1, max_quality=5,\n",
    "                         use_cover=True, vancouver_steps=10):\n",
    "    # generate each statistic for num_runs trials\n",
    "    means_acc = []\n",
    "    medians_acc = []\n",
    "    maxes_acc = []\n",
    "    for _ in range(num_runs):\n",
    "        errors = evaluate_vancouver(num_assignments, num_reviews, num_truths, min_quality, max_quality,\n",
    "                                    use_cover, vancouver_steps)\n",
    "        means = [np.mean(stat) for stat in errors]\n",
    "        means_acc.append(means)\n",
    "        medians = [np.median(stat) for stat in errors]\n",
    "        medians_acc.append(medians)\n",
    "        maxes = [max(stat) for stat in errors]\n",
    "        maxes_acc.append(maxes)\n",
    "    \n",
    "    # average the results of the statistics across the trials\n",
    "    mean_average = np.mean(means_acc, axis=0)\n",
    "    median_average = np.mean(medians_acc, axis=0)\n",
    "    max_average = np.mean(maxes_acc, axis=0)\n",
    "    \n",
    "    # tidy up the output for user-friendliness\n",
    "    mean_dict = {'sub_grade': mean_average[0], 'sub_var': mean_average[1], 'usr_var': mean_average[2]}\n",
    "    median_dict = {'sub_grade': median_average[0], 'sub_var': median_average[1], 'usr_var': median_average[2]}\n",
    "    max_dict = {'sub_grade': max_average[0], 'sub_var': max_average[1], 'usr_var': max_average[2]}\n",
    "    \n",
    "    return {'mean': mean_dict, 'median': median_dict, 'max': max_dict}\n",
    "\n",
    "def print_stats(stats):\n",
    "    print('Expectation of the Error in a Given Trial', '\\n')\n",
    "    \n",
    "    print('Assignment Grades:')\n",
    "    print('Mean Error: ', stats['mean']['sub_grade'])\n",
    "    print('Maximum Error: ', stats['max']['sub_grade'])\n",
    "    print('Median Error: ', stats['median']['sub_grade'], '\\n')\n",
    "    \n",
    "    print('Assignment Variances:')\n",
    "    print('Mean Error: ', stats['mean']['sub_var'])\n",
    "    print('Maximum Error: ', stats['max']['sub_var'])\n",
    "    print('Median Error: ', stats['median']['sub_var'], '\\n')\n",
    "    \n",
    "    print('Grader Variances:')\n",
    "    print('Mean Error: ', stats['mean']['usr_var'])\n",
    "    print('Maximum Error: ', stats['max']['usr_var'])\n",
    "    print('Median Error: ', stats['median']['usr_var'], '\\n', '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print_stats(vancouver_statistics(20, 3, 10, 3, use_cover=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_stats(stat_type, stat_variable, min_quality=1, max_quality=5, use_cover=True,\n",
    "               vancouver_steps=10, num_subs=20, num_grades_per_sub=3, num_trials=10, step_size=1):\n",
    "    stats = []\n",
    "    for num_true_grades in range(0, num_subs + step_size, step_size):\n",
    "        #print(num_true_grades)\n",
    "        stats.append(vancouver_statistics(num_subs, num_grades_per_sub, num_true_grades, num_trials, min_quality,\n",
    "                                          max_quality, use_cover)[stat_type][stat_variable])\n",
    "    \n",
    "    plt.plot(range(0, num_subs + step_size, step_size), stats)\n",
    "    plt.xlabel('Number of Ground-Truth Grades')\n",
    "    plt.ylabel(stat_type + ' ' + stat_variable + ' Error')\n",
    "    plt.title('Quality ranging from ' + str(min_quality) + ' to ' + str(max_quality))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_stats('mean', 'sub_grade', min_quality=1, max_quality=5, num_trials=1, use_cover=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a working plot function and another function which can display specific stats. This should be enough to accumulate reasonable amounts of data. I want to start with an examination of what happens without a cover as we increase the number of ground truth grades with graders at different variances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_stats('mean', 'sub_grade', min_quality=1, max_quality=5, num_trials=10, use_cover=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That appears to decrease linearly, but I am not convinced, so let's run it with a higher number of trials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_stats('mean', 'sub_grade', min_quality=1, max_quality=5, num_trials=100, use_cover=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This more precise examination shows a higher-order curve of some sort, albeit a very slight one. We had hoped this effect would be greater, but it looks like the actual amount of ground truth added in has much more effect than the \"ripples\" of this information (if there were no secondary effects, the graph would be a straight linear decrease). This may imply that the number of TA grades needed scales linearly with the accuracy desired."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with Covers\n",
    "\n",
    "My next step will be to look at whether assigning graders to a cover causes a deflection of any sort in the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_stats('mean', 'sub_grade', min_quality=1, max_quality=5, num_trials=25, use_cover=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This result appears to be interesting enough to warrant a high-resolution version of the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_stats('mean', 'sub_grade', min_quality=1, max_quality=5, num_trials=100, use_cover=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
