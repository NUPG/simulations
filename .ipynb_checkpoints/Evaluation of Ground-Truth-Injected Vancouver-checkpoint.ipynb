{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from peer_review import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def evaluate_vancouver(num_assignments, num_reviews, num_truths):\n",
    "    # generate random groups, assignments, and reviews\n",
    "    groups = { sub : [sub + x for x in ['1','2','3']] for sub in [ chr(ord('a') + z) for z in range(num_assignments)]}\n",
    "    assignments = peer_assignment(groups, num_reviews)\n",
    "    reviews = random_reviews(assignments)\n",
    "    \n",
    "    # generate a random ground truth value for all submissions\n",
    "    truths = {i: random.random() for i in groups}\n",
    "    truths_visible = truths.copy()\n",
    "    \n",
    "    # make a truths_visible dictionary for the algorithm to have access to\n",
    "    truths_visible = truths.copy()\n",
    "    _ = {truths_visible.popitem() for i in range(num_assignments - num_truths)}\n",
    "    \n",
    "    # run vancouver and omniscient vancouver\n",
    "    scores, qualities = vancouver(reviews, truths_visible, 1000)\n",
    "    omni_scores, omni_qualities = vancouver(reviews, truths, 1000)\n",
    "    \n",
    "    \n",
    "    # generate statistics on the data\n",
    "\n",
    "    '''The bug'''\n",
    "    \n",
    "    #this should be the same as the commented code, but isn't\n",
    "    #sub_score_error = {scores[submission][0] - omni_scores[submission][0] for submission in scores}\n",
    "    \n",
    "    sub_score_error = []\n",
    "    for submission in scores:\n",
    "        sub_score_error.append(scores[submission][0] - omni_scores[submission][0])\n",
    "    \n",
    "    '''/ The Bug '''\n",
    "          \n",
    "    sub_var_error = {scores[submission][1] - omni_scores[submission][1] for submission in scores}\n",
    "    grader_var_error = {qualities[grader] - omni_qualities[grader] for grader in qualities}\n",
    "    \n",
    "    # perform a sanity check (shouldn't be necessary, but the interpreter is buggy)\n",
    "    assert len(sub_score_error) == num_assignments\n",
    "    assert len(sub_var_error) == num_assignments\n",
    "    assert len(grader_var_error) == num_assignments * num_reviews\n",
    "    \n",
    "    return sub_score_error, sub_var_error, grader_var_error\n",
    "    \n",
    "sub_score_error, sub_var_error, grader_var_error = evaluate_vancouver(20, 3, 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
