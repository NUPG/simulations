{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named simulations",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-7285325f1951>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpprint\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpprint\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msimulations\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named simulations"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from pprint import pprint\n",
    "from simulations import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def evaluate_vancouver(num_assignments, num_reviews, num_truths, min_quality=1, max_quality=5, use_cover=True,\n",
    "                       vancouver_steps=10):\n",
    "    # generate random groups, assignments, qualities, and reviews\n",
    "    groups = { sub : [sub + x for x in ['1','2','3']] for sub in [ chr(ord('a') + z) for z in range(num_assignments)]}\n",
    "    assignments, cover = peer_assignment(groups, num_reviews)\n",
    "    true_qualities = {i: random.randint(min_quality, max_quality) for i in assignments}\n",
    "    reviews = random_reviews(assignments, true_qualities)\n",
    "    \n",
    "    # generate a random ground truth value for all submissions\n",
    "    truths = {i: 0.5 for i in groups}\n",
    "    \n",
    "    # make a truths_visible dictionary for the algorithm to have access to\n",
    "    if use_cover:\n",
    "        if len(cover) > num_truths:\n",
    "            truths_visible = {i[0]: 0.5 for i in random.sample(list(cover), num_truths)}\n",
    "        else:\n",
    "            truths_visible = {i: 0.5 for i in cover}\n",
    "            while len(truths_visible.keys()) < num_truths:\n",
    "                truths_visible[random.choice(truths)] = 0.5\n",
    "    else:\n",
    "        truths_visible = {i[0]: 0.5 for i in random.sample(list(truths), num_truths)}\n",
    "    \n",
    "    # run vancouver and omniscient vancouver\n",
    "    scores, qualities = vancouver(reviews, truths_visible, vancouver_steps)\n",
    "    omni_scores, omni_qualities = vancouver(reviews, truths, vancouver_steps)\n",
    "    \n",
    "    # generate statistics on the data\n",
    "    sub_score_error = [abs(scores[submission][0] - 0.5) for submission in scores]\n",
    "    sub_var_error = [abs(scores[submission][1] - omni_scores[submission][1]) for submission in scores]\n",
    "    grader_var_error = [abs(qualities[grader] - true_qualities[grader]) for grader in qualities]\n",
    "\n",
    "    return sub_score_error, sub_var_error, grader_var_error\n",
    "\n",
    "    \n",
    "def vancouver_statistics(num_assignments, num_reviews, num_truths, num_runs, min_quality=1, max_quality=5,\n",
    "                         use_cover=True, vancouver_steps=10):\n",
    "    # generate each statistic for num_runs trials\n",
    "    means_acc = []\n",
    "    medians_acc = []\n",
    "    maxes_acc = []\n",
    "    for _ in range(num_runs):\n",
    "        errors = evaluate_vancouver(num_assignments, num_reviews, num_truths, min_quality, max_quality,\n",
    "                                    use_cover, vancouver_steps)\n",
    "        means = [np.mean(stat) for stat in errors]\n",
    "        means_acc.append(means)\n",
    "        medians = [np.median(stat) for stat in errors]\n",
    "        medians_acc.append(medians)\n",
    "        maxes = [max(stat) for stat in errors]\n",
    "        maxes_acc.append(maxes)\n",
    "    \n",
    "    # average the results of the statistics across the trials\n",
    "    mean_average = np.mean(means_acc, axis=0)\n",
    "    median_average = np.mean(medians_acc, axis=0)\n",
    "    max_average = np.mean(maxes_acc, axis=0)\n",
    "    \n",
    "    # tidy up the output for user-friendliness\n",
    "    mean_dict = {'sub_grade': mean_average[0], 'sub_var': mean_average[1], 'usr_var': mean_average[2]}\n",
    "    median_dict = {'sub_grade': median_average[0], 'sub_var': median_average[1], 'usr_var': median_average[2]}\n",
    "    max_dict = {'sub_grade': max_average[0], 'sub_var': max_average[1], 'usr_var': max_average[2]}\n",
    "    \n",
    "    return {'mean': mean_dict, 'median': median_dict, 'max': max_dict}\n",
    "\n",
    "def print_stats(stats):\n",
    "    print('Expectation of the Error in a Given Trial', '\\n')\n",
    "    \n",
    "    print('Assignment Grades:')\n",
    "    print('Mean Error: ', stats['mean']['sub_grade'])\n",
    "    print('Maximum Error: ', stats['max']['sub_grade'])\n",
    "    print('Median Error: ', stats['median']['sub_grade'], '\\n')\n",
    "    \n",
    "    print('Assignment Variances:')\n",
    "    print('Mean Error: ', stats['mean']['sub_var'])\n",
    "    print('Maximum Error: ', stats['max']['sub_var'])\n",
    "    print('Median Error: ', stats['median']['sub_var'], '\\n')\n",
    "    \n",
    "    print('Grader Variances:')\n",
    "    print('Mean Error: ', stats['mean']['usr_var'])\n",
    "    print('Maximum Error: ', stats['max']['usr_var'])\n",
    "    print('Median Error: ', stats['median']['usr_var'], '\\n', '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print_stats(vancouver_statistics(20, 3, 5, 3, use_cover=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_stats(stat_type, stat_variable, min_quality=1, max_quality=5, use_cover=True,\n",
    "               vancouver_steps=10, num_subs=20, num_grades_per_sub=3, num_trials=10, step_size=1):\n",
    "    stats = []\n",
    "    for num_true_grades in range(0, num_subs + step_size, step_size):\n",
    "        #print(num_true_grades)\n",
    "        stats.append(vancouver_statistics(num_subs, num_grades_per_sub, num_true_grades, num_trials, min_quality,\n",
    "                                          max_quality, use_cover)[stat_type][stat_variable])\n",
    "    \n",
    "    plt.plot(range(0, num_subs + step_size, step_size), stats)\n",
    "    plt.xlabel('Number of Ground-Truth Grades')\n",
    "    plt.ylabel(stat_type + ' ' + stat_variable + ' Error')\n",
    "    plt.title('Quality ranging from ' + str(min_quality) + ' to ' + str(max_quality))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_stats('mean', 'sub_grade', min_quality=1, max_quality=5, num_trials=1, use_cover=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vancouver With Injected Ground Truth\n",
    "\n",
    "We now have a working plot function and another function which can display specific stats. This should be enough to accumulate reasonable amounts of data. I want to start with an examination of what happens without a cover as we increase the number of ground truth grades with graders at different variances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_stats('mean', 'sub_grade', min_quality=1, max_quality=5, num_trials=10, use_cover=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That appears to decrease linearly, but I am not convinced, so let's run it with a higher number of trials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_stats('mean', 'sub_grade', min_quality=1, max_quality=5, num_trials=100, use_cover=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This does indeed show that the plot is linear without the use of covers. Any effects the TA grades have on the non-TA-graded submissions must be minimal, which is slightly disappointing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with Covers\n",
    "\n",
    "My next step will be to look at whether assigning graders to a cover causes a deflection of any sort in the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_stats('mean', 'sub_grade', min_quality=1, max_quality=5, num_trials=25, use_cover=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, this appears linear, but I want to run a high-resolution plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_stats('mean', 'sub_grade', min_quality=1, max_quality=5, num_trials=100, use_cover=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This is pretty conclusive evidence that unless the code I am running is wrong (which is a possibility, despite my best efforts), the error mean submission grade error converges to zero linearly as ground truth grades increase. This means, unfortunately, that the error scales down linearly with time and effort, and there is no real \"tipping point\" of how much effort should be put in. This is actually a somewhat surprising result, since it means that Vancouver is not really able to use the information from ground truth very effectively. This may change when more people are grading the same assignment, so that is what I want to try next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_stats('mean', 'sub_grade', min_quality=1, max_quality=5, num_trials=20, use_cover=True, num_grades_per_sub=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That about halves the error, which is what I would expect since each assignment gets double the number of input grades. It's still linear, or too close to linear to be reliably distinguished from it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verification of Vancouver\n",
    "\n",
    "I want to take a moment out now to verify that I am using the correct number of Vancouver steps, just to be sure that these results are valid. I can do that by comparing plots at different numbers of iterations, to see if the error at all points is lower or the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_stats('mean', 'sub_grade', min_quality=1, max_quality=5, num_trials=20, use_cover=True, vancouver_steps=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doubling the number of Vancouver iterations doesn't appear to have done anything to the plot, so from now on I'm going to assume that for this sample size at least, ten iterations is sufficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of Results\n",
    "\n",
    "I have conducted simulations in several areas. First, I wanted to verify Vancouver, and this was easy. For a sample size of twenty students grading three submissions each, ten iterations appears to be sufficient to allow the algorithm to appropriately converge. Second, I wanted to determine the manner in which injection of ground truth grades would cause the error to change. I have found that said injection causes the mean error to decrease linearly, which implies that non-ground-truth assignments are not affected by ground truth injection in a meaningful way. Third, I attempted to determine if the use of a planted cover would be different than not using a planted cover in this regard, to see if we might gain bonus benefits by having graded a cover. I was able to discern no such difference; the plots appear to be identical.\n",
    "\n",
    "For the purpose of grading assignments, this means that the work required to be done by the TAs is linear with the error considered allowable by the instructors. The maximum error is even worse, decreasing slowly until it drops off sharply near the end, though this can be rendered largely irrelevant by the presence of an appeals process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
